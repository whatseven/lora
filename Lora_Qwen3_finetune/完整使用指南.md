# Qwen3 LoRAå¾®è°ƒå®Œæ•´ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—æä¾›äº†Qwen3æ¨¡å‹LoRAå¾®è°ƒé¡¹ç›®çš„å®Œæ•´ä½¿ç”¨è¯´æ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€ç‰ˆæœ¬ç®¡ç†ã€æ€§èƒ½è¯„ä¼°ç­‰æ‰€æœ‰ç¯èŠ‚ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
Lora_Qwen3_finetune/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ pretrained/          # é¢„è®­ç»ƒæ¨¡å‹ç¼“å­˜
â”‚   â””â”€â”€ finetuned/          # å¾®è°ƒæ¨¡å‹å­˜å‚¨
â”‚       â”œâ”€â”€ lora_v20241205_143000/    # ç‰ˆæœ¬åŒ–ç›®å½•
â”‚       â”œâ”€â”€ lora_v20241205_150000/    
â”‚       â””â”€â”€ latest -> lora_v20241205_150000/  # æœ€æ–°ç‰ˆæœ¬çš„è½¯é“¾æ¥
â”œâ”€â”€ dataset/                # æ•°æ®é›† éœ€æ‰‹åŠ¨ä¿®æ”¹æ•°æ®é›†åç§°ä¸ºalpaca_train.json,æ”¾åœ¨datasetç›®å½•ä¸‹ï¼Œå…¶ä»–éƒ¨åˆ†çš„æ•°æ®è®­ç»ƒåä¹Ÿéœ€è¦ä¿®æ”¹åç§°ï¼Œä½¿å¾—åç»­è®­ç»ƒä¸å†²çª
â”œâ”€â”€ logs/                   # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ evaluation/             # è¯„ä¼°ç»“æœ
â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ benchmark.py           # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ model_manager.py       # æ¨¡å‹ç®¡ç†å·¥å…·
â””â”€â”€ run_benchmark.sh       # è¯„ä¼°å¯åŠ¨è„šæœ¬
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ¨¡å‹è®­ç»ƒ

#### åŸºç¡€è®­ç»ƒå‘½ä»¤
```bash
cd /home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/
conda activate qwen3_finetune
python train.py
```

#### åå°è®­ç»ƒï¼ˆæ¨èï¼‰
```bash
# å®Œæ•´è®­ç»ƒï¼ˆå¯èƒ½éœ€è¦å‡ å°æ—¶ï¼‰
nohup python train.py > logs/train_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
tail -f logs/train_*.log

# æŸ¥çœ‹æ‰€æœ‰åå°ä»»åŠ¡
jobs
```

### 2. æ¨¡å‹è¯„ä¼°

#### ğŸŒŸ æ€§èƒ½ä¼˜åŒ–æ¨¡å¼ï¼ˆæ¨èï¼‰

**å¿«é€Ÿè¯„ä¼°æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰**
```bash
# ä½¿ç”¨é»˜è®¤ä¼˜åŒ–è®¾ç½®ï¼šæ‰¹å¤„ç†å¤§å°2ï¼Œå…³é—­BERTScore
./run_benchmark.sh --sample_size 50

# åå°è¿è¡Œ
nohup ./run_benchmark.sh --sample_size 50 > logs/eval_fast.log 2>&1 &
```

**å®Œæ•´è¯„ä¼°æ¨¡å¼**
```bash
# å¯ç”¨BERTScoreè¿›è¡Œå®Œæ•´è¯„ä¼°
./run_benchmark.sh --enable_bertscore --sample_size 50

# åå°è¿è¡Œ
nohup ./run_benchmark.sh --enable_bertscore --sample_size 50 > logs/eval_full.log 2>&1 &
```

**è‡ªå®šä¹‰æ‰¹å¤„ç†å¤§å°**
```bash
# ä½¿ç”¨æ›´å¤§æ‰¹å¤„ç†ï¼ˆéœ€è¦æ›´å¤šGPUå†…å­˜ï¼‰
./run_benchmark.sh --batch_size 4 --sample_size 50

# ä½¿ç”¨æ›´å°æ‰¹å¤„ç†ï¼ˆæ›´èŠ‚çœå†…å­˜ï¼‰
./run_benchmark.sh --batch_size 1 --sample_size 50
```

**é«˜çº§ç»„åˆç¤ºä¾‹**
```bash
# é«˜æ€§èƒ½æ¨¡å¼ï¼šå¤§æ‰¹å¤„ç† + å…³é—­BERTScore
nohup ./run_benchmark.sh --batch_size 4 --sample_size 100 > logs/eval_fast_batch4.log 2>&1 &

# å¹³è¡¡æ¨¡å¼ï¼šä¸­ç­‰æ‰¹å¤„ç† + å¯ç”¨BERTScore  
nohup ./run_benchmark.sh --batch_size 2 --enable_bertscore --sample_size 100 > logs/eval_balanced.log 2>&1 &
```

#### ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | æ‰¹å¤„ç†å¤§å° | BERTScore | 50æ ·æœ¬é¢„ä¼°æ—¶é—´ | æ€§èƒ½æå‡ |
|------|-----------|-----------|---------------|----------|
| **å¿«é€Ÿæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰** | 2 | å…³é—­ | 30-40åˆ†é’Ÿ | 3-4x |
| **é«˜é€Ÿæ¨¡å¼** | 4 | å…³é—­ | 20-30åˆ†é’Ÿ | 5-6x |
| **å®Œæ•´æ¨¡å¼** | 2 | å¯ç”¨ | 60-90åˆ†é’Ÿ | 2-3x |
| **åŸå§‹æ¨¡å¼** | 1 | å¯ç”¨ | 2-3å°æ—¶ | 1x |

#### ğŸ¯ Sample Size è¯´æ˜

**å…³äºsample_sizeå‚æ•°çš„è¡Œä¸ºï¼š**
- å¦‚æœæµ‹è¯•é›†æœ‰99æ¡æ•°æ®ï¼Œè®¾ç½®`--sample_size 300`ï¼Œå®é™…ä¼šä½¿ç”¨å…¨éƒ¨99æ¡æ•°æ®
- å¦‚æœæµ‹è¯•é›†æœ‰500æ¡æ•°æ®ï¼Œè®¾ç½®`--sample_size 300`ï¼Œä¼šéšæœºé‡‡æ ·300æ¡æ•°æ®
- ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹© `min(actual_data_size, sample_size)` è¿›è¡Œè¯„ä¼°
- éšæœºé‡‡æ ·ä½¿ç”¨å›ºå®šç§å­(42)ï¼Œç¡®ä¿ç»“æœå¯å¤ç°

**ç¤ºä¾‹æ—¥å¿—è¾“å‡ºï¼š**
```
=== æ­¥éª¤2: åŠ è½½æµ‹è¯•æ•°æ® ===
æ€»æµ‹è¯•æ•°æ®é‡: 99
ä½¿ç”¨å…¨éƒ¨ 99 æ¡æ•°æ®è¿›è¡Œè¯„ä¼°
âœ… å®é™…è¯„ä¼°æ•°æ®é‡: 99
```

### 3. æ–­ç‚¹ç»­ä¼ 

#### è¯„ä¼°æ–­ç‚¹ç»­ä¼ 
```bash
# å¦‚æœè¯„ä¼°ä¸­æ–­ï¼Œå¯ä»¥ä»æ–­ç‚¹ç»§ç»­
./run_benchmark.sh --checkpoint evaluation/checkpoint.json --sample_size 50

# åå°è¿è¡Œæ–­ç‚¹ç»­ä¼ 
nohup ./run_benchmark.sh --checkpoint evaluation/checkpoint.json --sample_size 50 > logs/eval_resume.log 2>&1 &
```

## ğŸ› ï¸ é«˜çº§é…ç½®

### è®­ç»ƒé…ç½®ä¿®æ”¹

ç¼–è¾‘ `train.py` æ–‡ä»¶ä¸­çš„é…ç½®ï¼š

```305:318:train.py
# LoRAé…ç½®
lora_config = LoraConfig(
    r=16,                    # LoRAç§©
    lora_alpha=32,          # LoRA alphaå‚æ•°
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)
```

### è¯„ä¼°é…ç½®ä¿®æ”¹

ç¼–è¾‘ `benchmark.py` æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„é…ç½®ï¼š

```585:605:benchmark.py
# æ¨¡å‹é…ç½®é€‰é¡¹ï¼š
# é€‰é¡¹1: é¢„è®­ç»ƒæ¨¡å‹ vs æœ€æ–°å¾®è°ƒæ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
MODEL_A_PATH = None  # ä½¿ç”¨é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹
MODEL_B_PATH = None  # ä½¿ç”¨æœ€æ–°å¾®è°ƒæ¨¡å‹

# é€‰é¡¹2: é¢„è®­ç»ƒæ¨¡å‹ vs æŒ‡å®šå¾®è°ƒæ¨¡å‹
# MODEL_A_PATH = None
# MODEL_B_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_143000"

# é€‰é¡¹3: ä¸¤ä¸ªå¾®è°ƒç‰ˆæœ¬å¯¹æ¯”
# MODEL_A_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_143000"
# MODEL_B_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_150000"
```

## ğŸ” æ¨¡å‹ç®¡ç†

### æŸ¥çœ‹æ¨¡å‹ç‰ˆæœ¬
```bash
python model_manager.py
```

### æ¨¡å‹ç‰ˆæœ¬è¯´æ˜
- **ç‰ˆæœ¬ç›®å½•**: `lora_v{YYYYMMDD}_{HHMMSS}` æ ¼å¼ï¼Œå¦‚ `lora_v20241205_143000`
- **latesté“¾æ¥**: æ€»æ˜¯æŒ‡å‘æœ€æ–°è®­ç»ƒçš„æ¨¡å‹ç‰ˆæœ¬
- **æ ¸å¿ƒæ–‡ä»¶**: `adapter_model.safetensors` (245MB) åŒ…å«LoRAå‚æ•°

## ğŸ“Š ç»“æœæŸ¥çœ‹

### è¯„ä¼°ç»“æœæ–‡ä»¶
è¯„ä¼°å®Œæˆåï¼Œåœ¨ `evaluation/` ç›®å½•ä¸‹ä¼šç”Ÿæˆï¼š
- `evaluation_report_{timestamp}.md` - è¯¦ç»†æŠ¥å‘Š
- `comparison_{timestamp}.png` - æ€§èƒ½å¯¹æ¯”å›¾è¡¨
- `evaluation_results_{timestamp}.json` - åŸå§‹æ•°æ®

### æ—¥å¿—ç›‘æ§
```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/train_*.log

# æŸ¥çœ‹è¯„ä¼°æ—¥å¿—  
tail -f logs/eval_*.log

# æŸ¥çœ‹åå°ä»»åŠ¡
jobs
ps aux | grep python
```

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### å†…å­˜ç®¡ç†
- **æ‰¹å¤„ç†å¤§å°**: å»ºè®®ä»2å¼€å§‹ï¼Œæ ¹æ®GPUå†…å­˜è°ƒæ•´
- **Tesla V100-32GB**: å¯ä»¥ä½¿ç”¨batch_size=4
- **æ˜¾å­˜ä¸è¶³æ—¶**: é™ä½batch_sizeåˆ°1

### å…¼å®¹æ€§
- æ‰€æœ‰æ–°åŠŸèƒ½ä¸ç°æœ‰ `/model/finetuned/lora` ç›®å½•å®Œå…¨å…¼å®¹
- ç‰ˆæœ¬ç®¡ç†ä¸ä¼šå½±å“ç°æœ‰æ¨¡å‹æ–‡ä»¶
- `latest` é“¾æ¥ç¡®ä¿è„šæœ¬å§‹ç»ˆä½¿ç”¨æœ€æ–°æ¨¡å‹

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
1. **é¦–æ¬¡è¯„ä¼°**: ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ `--sample_size 10` éªŒè¯ç¯å¢ƒ
2. **æ—¥å¸¸è¯„ä¼°**: ä½¿ç”¨é»˜è®¤ä¼˜åŒ–æ¨¡å¼ `--sample_size 50`
3. **å®Œæ•´è¯„ä¼°**: ä»…åœ¨æœ€ç»ˆéªŒè¯æ—¶å¯ç”¨BERTScore
4. **æ‰¹é‡å®éªŒ**: ä½¿ç”¨é«˜é€Ÿæ¨¡å¼ `--batch_size 4`

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   ```bash
   # é™ä½æ‰¹å¤„ç†å¤§å°
   ./run_benchmark.sh --batch_size 1
   ```

2. **è¯„ä¼°ä¸­æ–­**
   ```bash
   # ä½¿ç”¨æ–­ç‚¹ç»­ä¼ 
   ./run_benchmark.sh --checkpoint evaluation/checkpoint.json
   ```

3. **BERTScoreè®¡ç®—å¤±è´¥**
   ```bash
   # å…³é—­BERTScoreä½¿ç”¨å¿«é€Ÿæ¨¡å¼
   ./run_benchmark.sh  # é»˜è®¤å·²å…³é—­
   ```

4. **æƒé™é—®é¢˜**
   ```bash
   chmod +x run_benchmark.sh
   ```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### GPUä½¿ç”¨ç‡ç›‘æ§
```bash
# å®æ—¶ç›‘æ§GPU
watch -n 1 nvidia-smi

# åå°è®°å½•GPUä½¿ç”¨æƒ…å†µ
nohup watch -n 5 nvidia-smi >> logs/gpu_monitor.log 2>&1 &
```

### è®­ç»ƒè¿›åº¦ç›‘æ§
```bash
# ç›‘æ§è®­ç»ƒæŸå¤±
grep "loss" logs/train_*.log | tail -10

# ç›‘æ§è¯„ä¼°è¿›åº¦
grep "æ ·æœ¬" logs/eval_*.log | tail -5
```

---

**æœ€åæ›´æ–°**: 2024-12-05  
**ç‰ˆæœ¬**: v2.1 - æ·»åŠ æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ 