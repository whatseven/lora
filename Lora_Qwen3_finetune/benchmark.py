#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è„šæœ¬
å¯¹æ¯”åŸå§‹æ¨¡å‹vså¾®è°ƒæ¨¡å‹çš„BLEU/ROUGE/BERTScoreæŒ‡æ ‡
"""

import json
import os
import random
import time
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import datetime

# æ¨¡å‹åº“ - unslothå¿…é¡»åœ¨transformersä¹‹å‰å¯¼å…¥
from unsloth import FastLanguageModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# è¯„ä¼°æŒ‡æ ‡åº“
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import score as bert_score

class ModelBenchmark:
    def __init__(self, 
                 model_a_path=None,  # æ¨¡å‹Aè·¯å¾„
                 model_b_path=None,  # æ¨¡å‹Bè·¯å¾„
                 cache_dir="/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/pretrained",
                 output_dir="/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/evaluation",
                 enable_bertscore=False,  # BERTScoreå¼€å…³
                 batch_size=2):  # æ‰¹å¤„ç†å¤§å°
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ - æ”¯æŒå¤šç§å¯¹æ¯”æ¨¡å¼
        
        Args:
            model_a_path: æ¨¡å‹Aè·¯å¾„ï¼ˆå¯ä»¥æ˜¯é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ï¼‰
            model_b_path: æ¨¡å‹Bè·¯å¾„ï¼ˆå¯ä»¥æ˜¯å¾®è°ƒæ¨¡å‹è·¯å¾„æˆ–å…¶ä»–æ¨¡å‹è·¯å¾„ï¼‰
            cache_dir: ç¼“å­˜ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        
        å¯¹æ¯”æ¨¡å¼:
        1. é¢„è®­ç»ƒ vs å¾®è°ƒ: model_a_path=None, model_b_path=None (é»˜è®¤)
        2. é¢„è®­ç»ƒ vs æŒ‡å®šå¾®è°ƒ: model_a_path=None, model_b_path="path/to/model"
        3. ä¸¤ä¸ªå¾®è°ƒç‰ˆæœ¬å¯¹æ¯”: model_a_path="path/to/v1", model_b_path="path/to/v2"
        """
        
        self.cache_dir = cache_dir
        self.output_dir = output_dir
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_a_path is None:
            # é»˜è®¤ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹
            self.model_a_path = "unsloth/Qwen3-14B-unsloth-bnb-4bit"
            self.model_a_is_pretrained = True
        else:
            self.model_a_path = model_a_path
            self.model_a_is_pretrained = not os.path.exists(model_a_path)  # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œè®¤ä¸ºæ˜¯æ¨¡å‹åç§°
        
        if model_b_path is None:
            # ä½¿ç”¨æœ€æ–°çš„å¾®è°ƒæ¨¡å‹
            self.model_b_path = self._get_latest_model_path()
            self.model_b_is_pretrained = False
        else:
            self.model_b_path = model_b_path
            self.model_b_is_pretrained = not os.path.exists(model_b_path)
        
        # ç”Ÿæˆè¾“å‡ºæ ‡è¯†
        self.output_suffix = self._generate_output_suffix()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model_a = None
        self.tokenizer_a = None
        self.model_b = None
        self.tokenizer_b = None
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.enable_bertscore = enable_bertscore
        self.batch_size = batch_size
        
        # è¯„ä¼°æŒ‡æ ‡
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        self.smoothing_function = SmoothingFunction().method1
        
        print(f"ğŸ”§ è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹A: {self.model_a_path}")
        print(f"æ¨¡å‹B: {self.model_b_path}")
        print(f"è¾“å‡ºæ ‡è¯†: {self.output_suffix}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _get_latest_model_path(self):
        """è·å–æœ€æ–°çš„å¾®è°ƒæ¨¡å‹è·¯å¾„"""
        base_dir = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned"
        
        # ä¼˜å…ˆä½¿ç”¨latestè½¯é“¾æ¥
        latest_link = os.path.join(base_dir, "latest")
        if os.path.exists(latest_link) and os.path.islink(latest_link):
            return latest_link
        
        # å¦‚æœæ²¡æœ‰latesté“¾æ¥ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„lora_v*æ–‡ä»¶å¤¹
        lora_dirs = []
        if os.path.exists(base_dir):
            for item in os.listdir(base_dir):
                if item.startswith("lora_v") and os.path.isdir(os.path.join(base_dir, item)):
                    lora_dirs.append(item)
        
        if lora_dirs:
            # æŒ‰åç§°æ’åºï¼Œå–æœ€æ–°çš„
            lora_dirs.sort(reverse=True)
            return os.path.join(base_dir, lora_dirs[0])
        
        # å…¼å®¹æ—§ç‰ˆæœ¬è·¯å¾„
        old_path = os.path.join(base_dir, "lora")
        if os.path.exists(old_path):
            return old_path
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›None
        return None
    
    def _generate_output_suffix(self):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶çš„åç¼€æ ‡è¯†"""
        def get_model_identifier(path, is_pretrained):
            if is_pretrained:
                return "pretrained"
            else:
                # æå–æ¨¡å‹ç‰ˆæœ¬å·
                basename = os.path.basename(path.rstrip('/'))
                if basename == "latest":
                    # è§£ælatesté“¾æ¥æŒ‡å‘çš„å®é™…ç›®å½•
                    if os.path.islink(path):
                        target = os.readlink(path)
                        basename = target
                return basename
        
        model_a_id = get_model_identifier(self.model_a_path, self.model_a_is_pretrained)
        model_b_id = get_model_identifier(self.model_b_path, self.model_b_is_pretrained)
        
        return f"{model_a_id}_vs_{model_b_id}"
    
    def load_models(self):
        """åŠ è½½ä¸¤ä¸ªå¯¹æ¯”æ¨¡å‹"""
        print("\n=== æ­¥éª¤1: åŠ è½½æ¨¡å‹ ===")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not self.model_a_is_pretrained and not os.path.exists(self.model_a_path):
            print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹A: {self.model_a_path}")
            return False
        
        if not self.model_b_is_pretrained and not os.path.exists(self.model_b_path):
            print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹B: {self.model_b_path}")
            return False
        
        try:
            # åŠ è½½æ¨¡å‹A
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹A: {self.model_a_path}")
            
            if self.model_a_is_pretrained:
                # ä»å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
                self.model_a, self.tokenizer_a = FastLanguageModel.from_pretrained(
                    model_name=self.model_a_path,
                    max_seq_length=2048,
                    load_in_4bit=True,
                    cache_dir=self.cache_dir
                )
            else:
                # ä»æœ¬åœ°è·¯å¾„åŠ è½½
                self.model_a, self.tokenizer_a = FastLanguageModel.from_pretrained(
                    model_name=self.model_a_path,
                    max_seq_length=2048,
                    load_in_4bit=True,
                )
            print("âœ… æ¨¡å‹AåŠ è½½å®Œæˆ")
            
            # åŠ è½½æ¨¡å‹B
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹B: {self.model_b_path}")
            
            if self.model_b_is_pretrained:
                # ä»å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
                self.model_b, self.tokenizer_b = FastLanguageModel.from_pretrained(
                    model_name=self.model_b_path,
                    max_seq_length=2048,
                    load_in_4bit=True,
                    cache_dir=self.cache_dir
                )
            else:
                # ä»æœ¬åœ°è·¯å¾„åŠ è½½
                self.model_b, self.tokenizer_b = FastLanguageModel.from_pretrained(
                    model_name=self.model_b_path,
                    max_seq_length=2048,
                    load_in_4bit=True,
                )
            print("âœ… æ¨¡å‹BåŠ è½½å®Œæˆ")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_test_data(self, test_file, sample_size=300):
        """
        åŠ è½½æµ‹è¯•æ•°æ®å¹¶é‡‡æ ·
        
        Args:
            test_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·æ•°é‡ï¼Œå¦‚æœæµ‹è¯•é›†å°äºæ­¤æ•°é‡åˆ™ä½¿ç”¨å…¨éƒ¨
        
        Returns:
            list: é‡‡æ ·åçš„æµ‹è¯•æ•°æ®
        """
        print(f"\n=== æ­¥éª¤2: åŠ è½½æµ‹è¯•æ•°æ® ===")
        
        if not os.path.exists(test_file):
            print(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®: {test_file}")
            return None
        
        # åŠ è½½æ•°æ®
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        print(f"æ€»æµ‹è¯•æ•°æ®é‡: {len(test_data)}")
        
        # é‡‡æ ·é€»è¾‘
        if len(test_data) <= sample_size:
            print(f"ä½¿ç”¨å…¨éƒ¨ {len(test_data)} æ¡æ•°æ®è¿›è¡Œè¯„ä¼°")
            sampled_data = test_data
        else:
            print(f"éšæœºé‡‡æ · {sample_size} æ¡æ•°æ®è¿›è¡Œè¯„ä¼°")
            random.seed(42)  # ç¡®ä¿å¯å¤ç°
            sampled_data = random.sample(test_data, sample_size)
        
        print(f"âœ… å®é™…è¯„ä¼°æ•°æ®é‡: {len(sampled_data)}")
        return sampled_data
    
    def generate_batch_responses(self, model, tokenizer, batch_instructions, batch_inputs, max_new_tokens=512):
        """
        æ‰¹é‡ç”Ÿæˆæ¨¡å‹å›å¤ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            model: è¦ä½¿ç”¨çš„æ¨¡å‹
            tokenizer: å¯¹åº”çš„åˆ†è¯å™¨
            batch_instructions: æŒ‡ä»¤åˆ—è¡¨
            batch_inputs: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            list: ç”Ÿæˆçš„å›å¤åˆ—è¡¨
        """
        try:
            # æ„å»ºæ‰¹é‡æç¤º
            batch_prompts = []
            for instruction, input_text in zip(batch_instructions, batch_inputs):
                if input_text.strip():
                    prompt = f"### æŒ‡ä»¤:\n{instruction}\n\n### è¾“å…¥:\n{input_text}\n\n### å›ç­”:\n"
                else:
                    prompt = f"### æŒ‡ä»¤:\n{instruction}\n\n### å›ç­”:\n"
                batch_prompts.append(prompt)
            
            # æ‰¹é‡ç¼–ç 
            inputs = tokenizer(
                batch_prompts,
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=1024
            ).to(model.device)
            
            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    temperature=1.0,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç æ‰¹é‡ç»“æœ
            responses = []
            for i, output in enumerate(outputs):
                # ç§»é™¤è¾“å…¥éƒ¨åˆ†
                input_length = inputs['input_ids'][i].shape[0]
                generated_tokens = output[input_length:]
                
                # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
                response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                response = response.strip()
                
                responses.append(response)
            
            return responses
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•ä¸ªç”Ÿæˆ: {e}")
            # å›é€€åˆ°å•ä¸ªç”Ÿæˆ
            responses = []
            for instruction, input_text in zip(batch_instructions, batch_inputs):
                response = self.generate_response(model, tokenizer, instruction, input_text, max_new_tokens)
                responses.append(response)
            return responses

    def generate_response(self, model, tokenizer, instruction, input_text="", max_new_tokens=512):
        """
        ç”Ÿæˆæ¨¡å‹å›å¤
        
        Args:
            model: æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            instruction: æŒ‡ä»¤
            input_text: è¾“å…¥æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        
        Returns:
            str: ç”Ÿæˆçš„å›å¤
        """
        # æ„é€ è¾“å…¥
        if input_text.strip():
            user_content = f"{instruction}\n{input_text}"
        else:
            user_content = instruction
        
        messages = [{"role": "user", "content": user_content}]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ç”Ÿæˆå›å¤
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        return generated_text
    
    def calculate_metrics(self, reference, hypothesis):
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            reference: å‚è€ƒç­”æ¡ˆ
            hypothesis: ç”Ÿæˆç­”æ¡ˆ
        
        Returns:
            dict: åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        # BLEU-4
        ref_tokens = reference.split()
        hyp_tokens = hypothesis.split()
        bleu_4 = sentence_bleu([ref_tokens], hyp_tokens, 
                              weights=(0.25, 0.25, 0.25, 0.25),
                              smoothing_function=self.smoothing_function)
        
        # ROUGE-L
        rouge_scores = self.rouge_scorer.score(reference, hypothesis)
        rouge_l = rouge_scores['rougeL'].fmeasure
        
        # BERTScore (å¯é€‰ï¼Œé»˜è®¤å…³é—­ä»¥æå‡é€Ÿåº¦)
        if self.enable_bertscore:
            try:
                P, R, F1 = bert_score([hypothesis], [reference], 
                                    lang='zh', verbose=False, device='cuda' if torch.cuda.is_available() else 'cpu')
                bert_score_f1 = F1.item()
            except Exception as e:
                # å¦‚æœBERTScoreè®¡ç®—å¤±è´¥ï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯å¹¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                print(f"âš ï¸ BERTScoreè®¡ç®—å¤±è´¥: {e}")
                bert_score_f1 = 0.0
        else:
            # BERTScoreå…³é—­ï¼Œä½¿ç”¨0.0ä½œä¸ºå ä½ç¬¦
            bert_score_f1 = 0.0
        
        return {
            'bleu_4': bleu_4,
            'rouge_l': rouge_l,
            'bert_score_f1': bert_score_f1
        }
    
    def evaluate_models(self, test_data, checkpoint_file=None):
        """
        è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            checkpoint_file: æ–­ç‚¹ç»­ä¼ æ–‡ä»¶
        
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        print(f"\n=== æ­¥éª¤3: æ¨¡å‹è¯„ä¼°ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼‰===")
        print(f"æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        print(f"BERTScore: {'å¯ç”¨' if self.enable_bertscore else 'å…³é—­'}")
        
        # å°è¯•åŠ è½½æ–­ç‚¹
        start_idx = 0
        results = []
        
        if checkpoint_file and os.path.exists(checkpoint_file):
            print(f"å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œæ­£åœ¨æ¢å¤...")
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
                results = checkpoint_data['results']
                start_idx = len(results)
            print(f"ä»ç¬¬ {start_idx + 1} æ¡æ•°æ®ç»§ç»­è¯„ä¼°")
        
        # è¯„ä¼°è¿›åº¦
        total_batches = (len(test_data) - start_idx + self.batch_size - 1) // self.batch_size
        with tqdm(total=len(test_data) - start_idx, initial=0, 
                 desc="è¯„ä¼°è¿›åº¦", unit="samples") as pbar:
            
            # æ‰¹å¤„ç†å¾ªç¯
            for batch_start in range(start_idx, len(test_data), self.batch_size):
                batch_end = min(batch_start + self.batch_size, len(test_data))
                batch_data = test_data[batch_start:batch_end]
                
                try:
                    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                    batch_instructions = []
                    batch_inputs = []
                    batch_references = []
                    batch_indices = []
                    
                    for i, item in enumerate(batch_data):
                        batch_instructions.append(item['instruction'])
                        batch_inputs.append(item.get('input', ''))
                        batch_references.append(item['output'])
                        batch_indices.append(batch_start + i)
                    
                    # æ‰¹é‡ç”Ÿæˆæ¨¡å‹Aå›å¤
                    model_a_responses = self.generate_batch_responses(
                        self.model_a, self.tokenizer_a,
                        batch_instructions, batch_inputs
                    )
                    
                    # æ‰¹é‡ç”Ÿæˆæ¨¡å‹Bå›å¤
                    model_b_responses = self.generate_batch_responses(
                        self.model_b, self.tokenizer_b,
                        batch_instructions, batch_inputs
                    )
                    
                    # å¤„ç†æ‰¹æ¬¡ç»“æœ
                    for i in range(len(batch_data)):
                        try:
                            # è®¡ç®—æŒ‡æ ‡
                            model_a_metrics = self.calculate_metrics(
                                batch_references[i], model_a_responses[i]
                            )
                            model_b_metrics = self.calculate_metrics(
                                batch_references[i], model_b_responses[i]
                            )
                            
                            # ä¿å­˜ç»“æœ
                            result = {
                                'index': batch_indices[i],
                                'instruction': batch_instructions[i],
                                'input': batch_inputs[i],
                                'reference': batch_references[i],
                                'model_a_response': model_a_responses[i],
                                'model_b_response': model_b_responses[i],
                                'model_a_metrics': model_a_metrics,
                                'model_b_metrics': model_b_metrics
                            }
                            results.append(result)
                            
                        except Exception as e:
                            print(f"\nâš ï¸  å¤„ç†ç¬¬ {batch_indices[i]} æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                            continue
                    
                    # æ›´æ–°è¿›åº¦
                    pbar.update(len(batch_data))
                    
                    # å®šæœŸä¿å­˜æ–­ç‚¹ï¼ˆæ¯10ä¸ªæ ·æœ¬ï¼‰
                    if len(results) % 10 == 0:
                        self.save_checkpoint(results, checkpoint_file or 
                                           os.path.join(self.output_dir, 'checkpoint.json'))
                    
                    # GPUç¼“å­˜æ¸…ç†ï¼ˆæ‰¹æ¬¡é—´ï¼‰
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                except Exception as e:
                    print(f"\nâš ï¸  æ‰¹æ¬¡ {batch_start}-{batch_end} å¤„ç†å¤±è´¥: {e}")
                    # å›é€€åˆ°å•ä¸ªå¤„ç†
                    for i, item in enumerate(batch_data):
                        try:
                            idx = batch_start + i
                            model_a_response = self.generate_response(
                                self.model_a, self.tokenizer_a,
                                item['instruction'], item.get('input', '')
                            )
                            model_b_response = self.generate_response(
                                self.model_b, self.tokenizer_b,
                                item['instruction'], item.get('input', '')
                            )
                            
                            model_a_metrics = self.calculate_metrics(item['output'], model_a_response)
                            model_b_metrics = self.calculate_metrics(item['output'], model_b_response)
                            
                            result = {
                                'index': idx,
                                'instruction': item['instruction'],
                                'input': item.get('input', ''),
                                'reference': item['output'],
                                'model_a_response': model_a_response,
                                'model_b_response': model_b_response,
                                'model_a_metrics': model_a_metrics,
                                'model_b_metrics': model_b_metrics
                            }
                            results.append(result)
                            pbar.update(1)
                            
                        except Exception as e2:
                            print(f"\nâš ï¸  å•ä¸ªå¤„ç†ç¬¬ {idx} æ¡æ•°æ®ä¹Ÿå¤±è´¥: {e2}")
                            continue
        
        # æ¸…ç†æ–­ç‚¹æ–‡ä»¶
        if checkpoint_file and os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} æ¡æ•°æ®")
        return results
    
    def save_checkpoint(self, results, checkpoint_file):
        """ä¿å­˜æ–­ç‚¹æ–‡ä»¶"""
        checkpoint_data = {
            'timestamp': datetime.now().isoformat(),
            'results': results
        }
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    def calculate_summary_stats(self, results):
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n=== æ­¥éª¤4: è®¡ç®—æ±‡æ€»ç»Ÿè®¡ ===")
        
        # æå–æŒ‡æ ‡
        model_a_bleu = [r['model_a_metrics']['bleu_4'] for r in results]
        model_a_rouge = [r['model_a_metrics']['rouge_l'] for r in results]
        model_a_bert = [r['model_a_metrics']['bert_score_f1'] for r in results]
        
        model_b_bleu = [r['model_b_metrics']['bleu_4'] for r in results]
        model_b_rouge = [r['model_b_metrics']['rouge_l'] for r in results]
        model_b_bert = [r['model_b_metrics']['bert_score_f1'] for r in results]
        
        # è®¡ç®—å¹³å‡å€¼
        summary = {
            'model_a': {
                'bleu_4': np.mean(model_a_bleu),
                'rouge_l': np.mean(model_a_rouge),
                'bert_score_f1': np.mean(model_a_bert),
                'name': os.path.basename(self.model_a_path.rstrip('/'))
            },
            'model_b': {
                'bleu_4': np.mean(model_b_bleu),
                'rouge_l': np.mean(model_b_rouge),
                'bert_score_f1': np.mean(model_b_bert),
                'name': os.path.basename(self.model_b_path.rstrip('/'))
            }
        }
        
        # è®¡ç®—å·®å¼‚ï¼ˆBç›¸å¯¹äºAçš„å˜åŒ–ï¼‰
        summary['difference'] = {
            'bleu_4': summary['model_b']['bleu_4'] - summary['model_a']['bleu_4'],
            'rouge_l': summary['model_b']['rouge_l'] - summary['model_a']['rouge_l'],
            'bert_score_f1': summary['model_b']['bert_score_f1'] - summary['model_a']['bert_score_f1']
        }
        
        # è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–
        def safe_percentage(diff, base):
            return (diff / base * 100) if base != 0 else 0.0
        
        summary['percentage_change'] = {
            'bleu_4': safe_percentage(summary['difference']['bleu_4'], summary['model_a']['bleu_4']),
            'rouge_l': safe_percentage(summary['difference']['rouge_l'], summary['model_a']['rouge_l']),
            'bert_score_f1': safe_percentage(summary['difference']['bert_score_f1'], summary['model_a']['bert_score_f1'])
        }
        
        print("âœ… ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
        return summary
    
    def create_visualization(self, summary):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print(f"\n=== æ­¥éª¤5: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        
        metrics = ['BLEU-4', 'ROUGE-L', 'BERTScore-F1']
        model_a_scores = [
            summary['model_a']['bleu_4'],
            summary['model_a']['rouge_l'],
            summary['model_a']['bert_score_f1']
        ]
        model_b_scores = [
            summary['model_b']['bleu_4'],
            summary['model_b']['rouge_l'],
            summary['model_b']['bert_score_f1']
        ]
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        x = np.arange(len(metrics))
        width = 0.35
        
        fig, ax = plt.subplots(figsize=(10, 6))
        bars1 = ax.bar(x - width/2, model_a_scores, width, label=f'æ¨¡å‹A ({summary["model_a"]["name"]})', alpha=0.8)
        bars2 = ax.bar(x + width/2, model_b_scores, width, label=f'æ¨¡å‹B ({summary["model_b"]["name"]})', alpha=0.8)
        
        ax.set_ylabel('åˆ†æ•°')
        ax.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics)
        ax.legend()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.annotate(f'{height:.3f}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨ï¼ˆåŒ…å«ç‰ˆæœ¬æ ‡è¯†ï¼‰
        chart_path = os.path.join(self.output_dir, f'comparison_{self.output_suffix}.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chart_path}")
        return chart_path
    
    def generate_report(self, summary, results, chart_path):
        """ç”ŸæˆMarkdownè¯„ä¼°æŠ¥å‘Š"""
        print(f"\n=== æ­¥éª¤6: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š ===")
        
        report_content = f"""# æ¨¡å‹å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
è¯„ä¼°æ ·æœ¬æ•°: {len(results)}

## æ¨¡å‹ä¿¡æ¯

- **æ¨¡å‹A**: {summary['model_a']['name']} ({self.model_a_path})
- **æ¨¡å‹B**: {summary['model_b']['name']} ({self.model_b_path})

## æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | BLEU-4 | ROUGE-L | BERTScore-F1 |
|------|--------|---------|-------------|
| {summary['model_a']['name']} | {summary['model_a']['bleu_4']:.3f} | {summary['model_a']['rouge_l']:.3f} | {summary['model_a']['bert_score_f1']:.3f} |
| {summary['model_b']['name']} | **{summary['model_b']['bleu_4']:.3f}** | **{summary['model_b']['rouge_l']:.3f}** | **{summary['model_b']['bert_score_f1']:.3f}** |

## å…³é”®ç»“è®º

ğŸ“Š **æ¨¡å‹Bç›¸å¯¹äºæ¨¡å‹Açš„æ€§èƒ½å˜åŒ–**ï¼š
- BLEU-4: {summary['difference']['bleu_4']:+.3f} ({summary['percentage_change']['bleu_4']:+.1f}%)
- ROUGE-L: {summary['difference']['rouge_l']:+.3f} ({summary['percentage_change']['rouge_l']:+.1f}%)
- BERTScore-F1: {summary['difference']['bert_score_f1']:+.3f} ({summary['percentage_change']['bert_score_f1']:+.1f}%)

## æ€§èƒ½åˆ†æ

### BLEU-4 åˆ†æ
BLEU-4è¡¡é‡ç”Ÿæˆæ–‡æœ¬çš„æµç•…åº¦å’Œå‡†ç¡®æ€§ã€‚æ¨¡å‹Bç›¸å¯¹äºæ¨¡å‹Aå¾—åˆ†{"æå‡" if summary['difference']['bleu_4'] > 0 else "ä¸‹é™"}äº†{abs(summary['difference']['bleu_4']):.3f}ï¼Œ
è¡¨æ˜æ¨¡å‹B{"æœ‰æ•ˆæ”¹å–„" if summary['difference']['bleu_4'] > 0 else "å¯èƒ½å½±å“"}äº†ç”Ÿæˆè´¨é‡ã€‚

### ROUGE-L åˆ†æ
ROUGE-Lè¡¡é‡ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒç­”æ¡ˆçš„æœ€é•¿å…¬å…±å­åºåˆ—åŒ¹é…åº¦ã€‚æ¨¡å‹Bç›¸å¯¹äºæ¨¡å‹Aå¾—åˆ†{"æå‡" if summary['difference']['rouge_l'] > 0 else "ä¸‹é™"}äº†{abs(summary['difference']['rouge_l']):.3f}ï¼Œ
è¯´æ˜æ¨¡å‹B{"æ›´å¥½åœ°" if summary['difference']['rouge_l'] > 0 else "ç›¸æ¯”æ¨¡å‹Aåœ¨"}æ•æ‰å…³é”®ä¿¡æ¯æ–¹é¢{"æœ‰æ”¹å–„" if summary['difference']['rouge_l'] > 0 else "æœ‰å·®å¼‚"}ã€‚

### BERTScore-F1 åˆ†æ
BERTScore-F1è¡¡é‡è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ŒæŠ—å™ªéŸ³èƒ½åŠ›æ›´å¼ºã€‚æ¨¡å‹Bç›¸å¯¹äºæ¨¡å‹Aå¾—åˆ†{"æå‡" if summary['difference']['bert_score_f1'] > 0 else "ä¸‹é™"}äº†{abs(summary['difference']['bert_score_f1']):.3f}ï¼Œ
åæ˜ å‡ºæ¨¡å‹Båœ¨è¯­ä¹‰ç†è§£èƒ½åŠ›æ–¹é¢{"æœ‰æ‰€æå‡" if summary['difference']['bert_score_f1'] > 0 else "æœ‰æ‰€ä¸åŒ"}ã€‚

## æ•°æ®æ–‡ä»¶

- ğŸ“Š [è¯¦ç»†è¯„ä¼°æ•°æ®](evaluation_results_{self.output_suffix}.json)
- ğŸ“ˆ [æ€§èƒ½å¯¹æ¯”å›¾]({os.path.basename(chart_path)})

---
*æ­¤æŠ¥å‘Šç”± Qwen3 æ¨¡å‹è¯„ä¼°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*å¯¹æ¯”æ ‡è¯†: {self.output_suffix}*
"""
        
        # ä¿å­˜æŠ¥å‘Šï¼ˆåŒ…å«ç‰ˆæœ¬æ ‡è¯†ï¼‰
        report_path = os.path.join(self.output_dir, f'evaluation_report_{self.output_suffix}.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
    
    def save_detailed_results(self, results, summary):
        """ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆåŒ…å«ç‰ˆæœ¬æ ‡è¯†ï¼‰
        detailed_path = os.path.join(self.output_dir, f'evaluation_results_{self.output_suffix}.json')
        output_data = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'total_samples': len(results),
                'model_a': self.model_a_path,
                'model_b': self.model_b_path,
                'comparison_id': self.output_suffix
            },
            'summary': summary,
            'detailed_results': results
        }
        
        with open(detailed_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_path}")
        return detailed_path

def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("ğŸš€ Qwen3æ¨¡å‹å¯¹æ¯”è¯„ä¼°å¼€å§‹")
    print("=" * 60)
    
    # ===============================================
    # ğŸ¯ æ¨¡å‹å¯¹æ¯”é…ç½® - åœ¨è¿™é‡Œä¿®æ”¹è¦å¯¹æ¯”çš„æ¨¡å‹
    # ===============================================
    
    # æ¨¡å‹é…ç½®é€‰é¡¹ï¼š
    # é€‰é¡¹1: é¢„è®­ç»ƒæ¨¡å‹ vs æœ€æ–°å¾®è°ƒæ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
    MODEL_A_PATH = None  # ä½¿ç”¨é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹
    MODEL_B_PATH = None  # ä½¿ç”¨æœ€æ–°å¾®è°ƒæ¨¡å‹
    
    # é€‰é¡¹2: é¢„è®­ç»ƒæ¨¡å‹ vs æŒ‡å®šå¾®è°ƒæ¨¡å‹
    # MODEL_A_PATH = None
    # MODEL_B_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_143000"
    
    # é€‰é¡¹3: ä¸¤ä¸ªå¾®è°ƒç‰ˆæœ¬å¯¹æ¯”
    # MODEL_A_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_143000"
    # MODEL_B_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_150000"
    
    # é€‰é¡¹4: ä½¿ç”¨latesté“¾æ¥
    # MODEL_A_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/latest"
    # MODEL_B_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241205_150000"
    
    print(f"ğŸ“Š å¯¹æ¯”é…ç½®:")
    print(f"  æ¨¡å‹A: {MODEL_A_PATH or 'é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹'}")
    print(f"  æ¨¡å‹B: {MODEL_B_PATH or 'æœ€æ–°å¾®è°ƒæ¨¡å‹'}")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    import argparse
    parser = argparse.ArgumentParser(description='Qwen3æ¨¡å‹å¯¹æ¯”è¯„ä¼°')
    parser.add_argument('--test_data', type=str, 
                       default='dataset/test/alpaca_test.json',
                       help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sample_size', type=int, default=300,
                       help='è¯„ä¼°æ ·æœ¬æ•°é‡')
    parser.add_argument('--checkpoint', type=str, 
                       help='æ–­ç‚¹æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--enable_bertscore', action='store_true', default=False,
                       help='å¯ç”¨BERTScoreè¯„ä¼°ï¼ˆé»˜è®¤å…³é—­ä»¥æå‡é€Ÿåº¦ï¼‰')
    parser.add_argument('--batch_size', type=int, default=2,
                       help='æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤2ï¼Œå‡å°‘GPUå†…å­˜å ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    benchmark = ModelBenchmark(
        model_a_path=MODEL_A_PATH,
        model_b_path=MODEL_B_PATH,
        enable_bertscore=args.enable_bertscore,
        batch_size=args.batch_size
    )
    
    print(f"âš¡ æ€§èƒ½ä¼˜åŒ–é…ç½®:")
    print(f"  BERTScore: {'å¯ç”¨' if args.enable_bertscore else 'å…³é—­ï¼ˆåŠ é€Ÿæ¨¡å¼ï¼‰'}")
    print(f"  æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        if not benchmark.load_models():
            return
        
        # 2. åŠ è½½æµ‹è¯•æ•°æ®
        test_data = benchmark.load_test_data(args.test_data, args.sample_size)
        if test_data is None:
            return
        
        # 3. æ‰§è¡Œè¯„ä¼°
        results = benchmark.evaluate_models(test_data, args.checkpoint)
        if not results:
            print("âŒ è¯„ä¼°å¤±è´¥ï¼Œæ²¡æœ‰è·å¾—æœ‰æ•ˆç»“æœ")
            return
        
        # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = benchmark.calculate_summary_stats(results)
        
        # 5. ç”Ÿæˆå¯è§†åŒ–
        chart_path = benchmark.create_visualization(summary)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report_path = benchmark.generate_report(summary, results, chart_path)
        
        # 7. ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_path = benchmark.save_detailed_results(results, summary)
        
        # 8. è¾“å‡ºæ€»ç»“
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“Š è¯„ä¼°æŠ¥å‘Š: {report_path}")
        print(f"ğŸ“ˆ å¯¹æ¯”å›¾è¡¨: {chart_path}")
        print(f"ğŸ“‹ è¯¦ç»†æ•°æ®: {detailed_path}")
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        print(f"\nğŸ“ˆ æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        print(f"BLEU-4: {summary['difference']['bleu_4']:+.3f} ({summary['percentage_change']['bleu_4']:+.1f}%)")
        print(f"ROUGE-L: {summary['difference']['rouge_l']:+.3f} ({summary['percentage_change']['rouge_l']:+.1f}%)")
        print(f"BERTScore-F1: {summary['difference']['bert_score_f1']:+.3f} ({summary['percentage_change']['bert_score_f1']:+.1f}%)")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return

if __name__ == "__main__":
    main() 