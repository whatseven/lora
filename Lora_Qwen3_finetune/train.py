#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-14B LoRAå¾®è°ƒä¸»è®­ç»ƒè„šæœ¬
ä½¿ç”¨unslothæ¡†æ¶è¿›è¡Œé«˜æ•ˆè®­ç»ƒ
"""

import os
import json
import torch
import pandas as pd
from datetime import datetime
from datasets import Dataset
from unsloth import FastLanguageModel
from trl import SFTTrainer, SFTConfig
from transformers import TextStreamer
from data_processor import process_for_training, load_alpaca_data

class Qwen3Trainer:
    def __init__(self, base_model_path=None, model_version=None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–ä¹‹å‰å¾®è°ƒçš„æ¨¡å‹è·¯å¾„
            model_version: æ¨¡å‹ç‰ˆæœ¬æ ‡è¯†ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç‰ˆæœ¬
        """
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        # è®¾ç½®åŸºç¡€æ¨¡å‹è·¯å¾„
        if base_model_path is None:
            # é»˜è®¤ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹
            self.base_model_path = "unsloth/Qwen3-14B-unsloth-bnb-4bit"
            self.is_pretrained = True
        else:
            # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹è·¯å¾„ï¼ˆå¯èƒ½æ˜¯ä¹‹å‰å¾®è°ƒçš„æ¨¡å‹ï¼‰
            self.base_model_path = base_model_path
            self.is_pretrained = False
        
        # è®¾ç½®æ¨¡å‹ç‰ˆæœ¬
        if model_version is None:
            # è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç‰ˆæœ¬
            self.model_version = datetime.now().strftime("%Y%m%d_%H%M%S")
        else:
            self.model_version = model_version
        
        # é…ç½®è·¯å¾„
        self.cache_dir = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/pretrained"
        self.output_base_dir = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned"
        self.output_dir = os.path.join(self.output_base_dir, f"lora_v{self.model_version}")
        self.log_dir = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/logs"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        print(f"ğŸ¯ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"åŸºç¡€æ¨¡å‹: {self.base_model_path}")
        print(f"æ¨¡å‹ç‰ˆæœ¬: {self.model_version}")
        print(f"è¾“å‡ºè·¯å¾„: {self.output_dir}")
    
    def setup_model(self, max_seq_length):
        """
        è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        æ”¯æŒä»é¢„è®­ç»ƒæ¨¡å‹æˆ–ä¹‹å‰å¾®è°ƒçš„æ¨¡å‹å¼€å§‹è®­ç»ƒ
        """
        print("=== æ­¥éª¤1: åŠ è½½åŸºç¡€æ¨¡å‹ ===")
        
        if self.is_pretrained:
            # è®¾ç½®ç¼“å­˜ç›®å½•ç¯å¢ƒå˜é‡
            os.environ["HF_HOME"] = self.cache_dir
            os.environ["TRANSFORMERS_CACHE"] = self.cache_dir
            print("ğŸ’¡ è¯·ç¡®ä¿è¿è¡Œå‰å·²è®¾ç½®: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {self.cache_dir}")
            print("é¦–æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.base_model_path}")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        if self.is_pretrained:
            # ä»å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=self.base_model_path,
                max_seq_length=max_seq_length,
                load_in_4bit=True,
                load_in_8bit=False,
                full_finetuning=False,
                cache_dir=self.cache_dir
            )
        else:
            # ä»ä¹‹å‰å¾®è°ƒçš„æ¨¡å‹åŠ è½½
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=self.base_model_path,
                max_seq_length=max_seq_length,
                load_in_4bit=True,
                load_in_8bit=False,
                full_finetuning=False,
            )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        return self.model, self.tokenizer
    
    def setup_lora(self, r, lora_alpha):
        """é…ç½®LoRAé€‚é…å™¨"""
        print("=== æ­¥éª¤2: é…ç½®LoRAé€‚é…å™¨ ===")
        print(f"LoRA rank: {r}, alpha: {lora_alpha}")
        
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=r,                      # A100 32Gå¯ä»¥ç”¨64
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=lora_alpha,    # é€šå¸¸ç­‰äºrank
            lora_dropout=0,           # 0è¡¨ç¤ºä¼˜åŒ–æ€§èƒ½
            bias="none",              # "none"æœ€ä¼˜åŒ–
            use_gradient_checkpointing="unsloth",  # èŠ‚çœ30%å†…å­˜
            random_state=3407,
            use_rslora=False,
            loftq_config=None,
        )
        
        print("âœ… LoRAé…ç½®å®Œæˆ")
        return self.model
    
    def load_datasets(self):
        """åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        print("=== æ­¥éª¤3: åŠ è½½æ•°æ®é›† ===")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        train_file = "dataset/train/alpaca_train.json"
        val_file = "dataset/valid/alpaca_valid.json"
        
        if not os.path.exists(train_file):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ® {train_file}")
            print("è¯·å…ˆè¿è¡Œ python data_processor.py å¤„ç†æ•°æ®é›†")
            return None, None
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        print("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
        train_data = load_alpaca_data(train_file)
        train_dataset = process_for_training(train_data, self.tokenizer)
        
        # åŠ è½½éªŒè¯æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        val_dataset = None
        if os.path.exists(val_file):
            print("æ­£åœ¨åŠ è½½éªŒè¯æ•°æ®...")
            val_data = load_alpaca_data(val_file)
            val_dataset = process_for_training(val_data, self.tokenizer)
        
        print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
        return train_dataset, val_dataset
    
    def setup_training(self, train_dataset, val_dataset, 
                      num_epochs, batch_size, learning_rate):
        """é…ç½®è®­ç»ƒå‚æ•°"""
        print("=== æ­¥éª¤4: é…ç½®è®­ç»ƒå‚æ•° ===")
        print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"å­¦ä¹ ç‡: {learning_rate}")
        
        self.trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            args=SFTConfig(
                dataset_text_field="text",
                per_device_train_batch_size=batch_size,
                gradient_accumulation_steps=4,  # æœ‰æ•ˆbatch size = 4*4=16
                warmup_steps=20,
                num_train_epochs=num_epochs,
                learning_rate=learning_rate,
                logging_steps=10,
                optim="adamw_8bit",
                weight_decay=0.01,
                lr_scheduler_type="linear",
                seed=3407,
                output_dir=self.output_dir,
                logging_dir=self.log_dir,
                report_to="none",               # å¯æ”¹ä¸º"wandb"
                save_strategy="epoch",
                save_steps=100,
                eval_strategy="epoch" if val_dataset else "no",
                eval_steps=50 if val_dataset else None,
                load_best_model_at_end=True if val_dataset else False,
                metric_for_best_model="eval_loss" if val_dataset else None,
                greater_is_better=False if val_dataset else None,
            ),
        )
        
        print("âœ… è®­ç»ƒé…ç½®å®Œæˆ")
        return self.trainer
    
    def show_gpu_stats(self):
        """æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            gpu_stats = torch.cuda.get_device_properties(0)
            start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
            max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
            print(f"GPU: {gpu_stats.name}")
            print(f"æœ€å¤§å†…å­˜: {max_memory} GB")
            print(f"å·²ç”¨å†…å­˜: {start_gpu_memory} GB")
            return start_gpu_memory, max_memory
        return 0, 0
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("=== æ­¥éª¤5: å¼€å§‹è®­ç»ƒ ===")
        
        # æ˜¾ç¤ºè®­ç»ƒå‰GPUçŠ¶æ€
        start_memory, max_memory = self.show_gpu_stats()
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ è®­ç»ƒå¼€å§‹...")
        trainer_stats = self.trainer.train()
        
        # æ˜¾ç¤ºè®­ç»ƒåGPUçŠ¶æ€
        if torch.cuda.is_available():
            used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
            used_memory_for_lora = round(used_memory - start_memory, 3)
            used_percentage = round(used_memory / max_memory * 100, 3)
            lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
            
            print(f"\n=== è®­ç»ƒå®Œæˆç»Ÿè®¡ ===")
            print(f"è®­ç»ƒæ—¶é—´: {trainer_stats.metrics['train_runtime']:.2f} ç§’")
            print(f"è®­ç»ƒæ—¶é—´: {round(trainer_stats.metrics['train_runtime']/60, 2)} åˆ†é’Ÿ")
            print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {used_memory} GB")
            print(f"è®­ç»ƒé¢å¤–å†…å­˜: {used_memory_for_lora} GB")
            print(f"å†…å­˜ä½¿ç”¨ç‡: {used_percentage}%")
            print(f"è®­ç»ƒå†…å­˜ä½¿ç”¨ç‡: {lora_percentage}%")
        
        print("âœ… è®­ç»ƒå®Œæˆ")
        return trainer_stats
    
    def save_model(self):
        """ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹"""
        print("=== æ­¥éª¤6: ä¿å­˜æ¨¡å‹ ===")
        print(f"ä¿å­˜è·¯å¾„: {self.output_dir}")
        
        # ä¿å­˜LoRAé€‚é…å™¨
        self.model.save_pretrained(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)
        
        # ä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯
        config_info = {
            "model_version": self.model_version,
            "base_model_path": self.base_model_path,
            "is_pretrained": self.is_pretrained,
            "save_time": datetime.now().isoformat(),
            "output_dir": self.output_dir
        }
        
        config_file = os.path.join(self.output_dir, "training_config.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_info, f, ensure_ascii=False, indent=2)
        
        print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
        print(f"LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_file}")
        
        # åˆ›å»ºæœ€æ–°æ¨¡å‹çš„è½¯é“¾æ¥
        latest_link = os.path.join(self.output_base_dir, "latest")
        if os.path.exists(latest_link):
            os.remove(latest_link)
        os.symlink(f"lora_v{self.model_version}", latest_link)
        print(f"âœ… æœ€æ–°æ¨¡å‹é“¾æ¥å·²åˆ›å»º: {latest_link}")
    
    def test_model(self, test_prompt="è¯·ä»‹ç»ä¸€ä¸‹é”‚ç”µæ± çš„åŸºæœ¬å·¥ä½œåŸç†å’Œä¸»è¦åº”ç”¨é¢†åŸŸ"):
        """ç®€å•æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹ - ä¿®æ”¹ä¸ºç”µæ± ç›¸å…³æµ‹è¯•é—®é¢˜"""
        print("=== æ­¥éª¤7: æ¨¡å‹æµ‹è¯• ===")
        print(f"æµ‹è¯•é—®é¢˜: {test_prompt}")
        
        # æ„é€ è¾“å…¥
        messages = [{"role": "user", "content": test_prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(text, return_tensors="pt").to("cuda")
        
        print("\næ¨¡å‹å›ç­”:")
        print("-" * 50)
        
        # ç”Ÿæˆå›å¤
        streamer = TextStreamer(self.tokenizer, skip_prompt=True)
        with torch.no_grad():
            _ = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.8,
                top_k=20,
                do_sample=True,
                streamer=streamer,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        print("-" * 50)
        print("âœ… æµ‹è¯•å®Œæˆ")

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("ğŸš€ Qwen3-14B LoRAå¾®è°ƒå¼€å§‹")
    print("=" * 60)
    
    # ===============================================
    # ğŸ¯ æ¨¡å‹è·¯å¾„é…ç½® - åœ¨è¿™é‡Œä¿®æ”¹åŸºç¡€æ¨¡å‹è·¯å¾„
    # ===============================================
    
    # åŸºç¡€æ¨¡å‹é…ç½®
    # é€‰é¡¹1: ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé¦–æ¬¡è®­ç»ƒï¼‰
    BASE_MODEL_PATH = None  # ä½¿ç”¨é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹
    
    # é€‰é¡¹2: ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹
    # BASE_MODEL_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/pretrained/Qwen3-14B"
    
    # é€‰é¡¹3: ä½¿ç”¨ä¹‹å‰å¾®è°ƒçš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆå¢é‡è®­ç»ƒï¼‰
    # BASE_MODEL_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/lora_v20241201_143000"
    # BASE_MODEL_PATH = "/home/ubuntu/ZJQ/ai_report/Lora_Qwen3_finetune/model/finetuned/latest"  # ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
    
    # æ¨¡å‹ç‰ˆæœ¬ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ï¼‰
    MODEL_VERSION = None  # ä¾‹å¦‚: "battery_v1" æˆ– None
    
    # ===============================================
    # ğŸ¯ æ‰€æœ‰è®­ç»ƒå‚æ•°éƒ½åœ¨è¿™é‡Œç»Ÿä¸€è®¾ç½®
    # ===============================================
    
    # æ¨¡å‹å‚æ•°
    MAX_SEQ_LENGTH = 1024          # åºåˆ—é•¿åº¦ (å¿«é€ŸéªŒè¯ç”¨1024ï¼Œæ­£å¼è®­ç»ƒç”¨2048)
    
    # LoRAå‚æ•° 
    LORA_RANK = 16                 # LoRA rank (å¿«é€ŸéªŒè¯ç”¨16ï¼Œæ­£å¼è®­ç»ƒç”¨64)
    LORA_ALPHA = 16                # LoRA alpha (é€šå¸¸ç­‰äºrank)
    
    # è®­ç»ƒå‚æ•°
    NUM_EPOCHS = 1                 # è®­ç»ƒè½®æ•° (å¿«é€ŸéªŒè¯ç”¨1ï¼Œæ­£å¼è®­ç»ƒç”¨3)
    BATCH_SIZE = 2                 # æ‰¹æ¬¡å¤§å° (å¿«é€ŸéªŒè¯ç”¨2ï¼Œæ­£å¼è®­ç»ƒç”¨4)
    LEARNING_RATE = 2e-4           # å­¦ä¹ ç‡
    
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  åŸºç¡€æ¨¡å‹: {BASE_MODEL_PATH or 'å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹'}")
    print(f"  æ¨¡å‹ç‰ˆæœ¬: {MODEL_VERSION or 'è‡ªåŠ¨æ—¶é—´æˆ³'}")
    print(f"  åºåˆ—é•¿åº¦: {MAX_SEQ_LENGTH}")
    print(f"  LoRA rank: {LORA_RANK}")
    print(f"  è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    print("=" * 60)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Qwen3Trainer(
        base_model_path=BASE_MODEL_PATH,
        model_version=MODEL_VERSION
    )
    
    try:
        # 1. è®¾ç½®æ¨¡å‹
        model, tokenizer = trainer.setup_model(max_seq_length=MAX_SEQ_LENGTH)
        
        # 2. é…ç½®LoRA
        model = trainer.setup_lora(r=LORA_RANK, lora_alpha=LORA_ALPHA)
        
        # 3. åŠ è½½æ•°æ®é›†
        train_dataset, val_dataset = trainer.load_datasets()
        if train_dataset is None:
            return
        
        # 4. é…ç½®è®­ç»ƒ
        trainer_obj = trainer.setup_training(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            num_epochs=NUM_EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE
        )
        
        # 5. å¼€å§‹è®­ç»ƒ
        trainer_stats = trainer.train()
        
        # 6. ä¿å­˜æ¨¡å‹
        trainer.save_model()
        
        # 7. æµ‹è¯•æ¨¡å‹
        trainer.test_model()
        
        print("\nğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
        print(f"æ¨¡å‹å·²ä¿å­˜åœ¨: {trainer.output_dir}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
        return

if __name__ == "__main__":
    main() 